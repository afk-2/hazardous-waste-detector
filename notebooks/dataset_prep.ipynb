{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1OQMzbgX4oKwF51A9o7WERZTb5XjdG961","authorship_tag":"ABX9TyNRTY5yYluTPCDgszAmnvPV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1TgV27YgPtjw"},"outputs":[],"source":["import os\n","import shutil\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","source":["dataset_path = \"/content/drive/MyDrive/Waste Classification Project/dataset\"\n","output_path = \"/content/drive/MyDrive/Waste Classification Project/dataset_processed\"\n","\n","# Load the image file paths from the hazardous and non-hazardous subfolders into separate lists.\n","hazardous_files = [os.path.join(dataset_path, \"hazardous\", f) for f in os.listdir(os.path.join(dataset_path, \"hazardous\"))]\n","non_hazardous_files = [os.path.join(dataset_path, \"non-hazardous\", f) for f in os.listdir(os.path.join(dataset_path, \"non-hazardous\"))]\n","\n","all_files = hazardous_files + non_hazardous_files\n","# Labels for each image (1 for hazardous, 0 for non-hazardous).\n","labels = [1] * len(hazardous_files) + [0] * len(non_hazardous_files)"],"metadata":{"id":"m8HdXRMJS7RE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split dataset into train, validation, and test (70% train, 15% validation, 15% test)\n","train_files, temp_files, train_labels, temp_labels = train_test_split(all_files, labels, test_size=0.3, stratify=labels, random_state=42)\n","val_files, test_files, val_labels, test_labels = train_test_split(temp_files, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42)"],"metadata":{"id":"OxoFKZwQcYSp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to copy files into appropriate folder (train, val, test)\n","def move_files(files, labels, split):\n","  for file, label in zip(files, labels):\n","    class_name = \"hazardous\" if label == 1 else \"non-hazardous\"\n","    split_path = os.path.join(output_path, split, class_name)\n","    os.makedirs(split_path, exist_ok=True)\n","    shutil.copy(file, split_path)"],"metadata":{"id":"OSdWcevjfBIp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Move files to respective folders (train, val, test)\n","move_files(train_files, train_labels, \"train\")\n","move_files(val_files, val_labels, \"val\")\n","move_files(test_files, test_labels, \"test\")"],"metadata":{"id":"IBRwcs4RuAnh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up ImageDataGenerator for augmentation and preprocessing\n","datagen = ImageDataGenerator(\n","    rescale=1./255,            # Normalize pixel values to between 0 and 1\n","    rotation_range=40,         # Random rotation range in degrees\n","    width_shift_range=0.2,     # Horizontal shift\n","    height_shift_range=0.2,    # Vertical shift\n","    shear_range=0.2,           # Shear transformation\n","    zoom_range=0.2,            # Zoom in/out\n","    horizontal_flip=True,      # Randomly flip images horizontally\n","    fill_mode='nearest'        # Fill missing pixels after transformations\n",")"],"metadata":{"id":"H6lL4B8lk9sB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply preprocessing and augmentation on the training set\n","train_generator = datagen.flow_from_directory(\n","    os.path.join(output_path, 'train'),\n","    target_size=(224, 224),  # Resize images to 224x224\n","    batch_size=32,\n","    class_mode='binary'      # Binary classification: hazardous vs non-hazardous\n",")"],"metadata":{"id":"QlJpDnfKlEbw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up a simple validation generator (no augmentation, only rescaling)\n","test_val_datagen = ImageDataGenerator(rescale=1./255)\n","val_generator = test_val_datagen.flow_from_directory(\n","    os.path.join(output_path, 'val'),\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='binary'\n",")\n","\n","# Test generator (no augmentation)\n","test_generator = test_val_datagen.flow_from_directory(\n","    os.path.join(output_path, 'test'),\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='binary'\n",")"],"metadata":{"id":"uR0C1qxZqgIq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Dataset preprocessing, augmentation, and splitting completed!\")"],"metadata":{"id":"-A8uNCZBrLBZ"},"execution_count":null,"outputs":[]}]}